Perfect â€” sounds like youâ€™ve got your **Amazon Bedrock Knowledge Base** working and tested successfully ðŸŽ¯

Now, to **integrate it with your application** so you can *send prompts and receive responses programmatically*, youâ€™ll use **the Bedrock Runtime API** (or SDK) and **the Knowledge Base Retrieval API**.

Letâ€™s go step-by-step ðŸ‘‡

---

## ðŸ§  Architecture Overview

Your app will:

1. **Send a query (prompt)** â†’ to the **Bedrock Retrieve API** to fetch context from your Knowledge Base (retrieval step).
2. **Feed that retrieved context + your userâ€™s prompt** â†’ to the **model (e.g., Claude, Nova, Titan)** through the **Bedrock Runtime API** to generate the final answer.

---

## ðŸ§© Step 1: Set up AWS SDK client

### Example: Node.js (AWS SDK v3)

```bash
npm install @aws-sdk/client-bedrock-runtime @aws-sdk/client-bedrock-agent-runtime
```

---

### In your app code:

```js
import {
  BedrockRuntimeClient,
  InvokeModelCommand
} from "@aws-sdk/client-bedrock-runtime";

import {
  BedrockAgentRuntimeClient,
  RetrieveCommand
} from "@aws-sdk/client-bedrock-agent-runtime";

const region = "us-east-1"; // adjust to your KB region

const runtimeClient = new BedrockRuntimeClient({ region });
const agentClient = new BedrockAgentRuntimeClient({ region });

// Your Knowledge Base ID
const knowledgeBaseId = "kb-xxxxxxxxxxxx";
```

---

## ðŸ§¾ Step 2: Retrieve context from your Knowledge Base

```js
async function retrieveContext(query) {
  const command = new RetrieveCommand({
    knowledgeBaseId,
    retrievalQuery: { text: query },
    retrievalConfiguration: { vectorSearchConfiguration: { numberOfResults: 5 } },
  });

  const response = await agentClient.send(command);

  // Extract relevant chunks
  const context = response.retrievalResults
    .map(r => r.content?.text)
    .join("\n\n");

  return context;
}
```

---

## ðŸ’¬ Step 3: Send the final prompt to the model

Example using **Claude 3 Haiku (or Nova Lite)**:

```js
async function generateAnswer(prompt, context) {
  const modelId = "anthropic.claude-3-haiku-20240307-v1:0"; // or nova.lite, etc.

  const command = new InvokeModelCommand({
    modelId,
    contentType: "application/json",
    accept: "application/json",
    body: JSON.stringify({
      messages: [
        {
          role: "user",
          content: `Use the following context to answer:\n\n${context}\n\nQuestion: ${prompt}`
        }
      ],
      max_tokens: 500,
    }),
  });

  const response = await runtimeClient.send(command);
  const output = JSON.parse(new TextDecoder().decode(response.body));
  return output.content[0].text;
}
```

---

## ðŸ§  Step 4: Full Example

```js
async function askKnowledgeBase(prompt) {
  const context = await retrieveContext(prompt);
  const answer = await generateAnswer(prompt, context);
  return answer;
}

askKnowledgeBase("What is the cargo load for flight AF129?")
  .then(console.log)
  .catch(console.error);
```

---

## ðŸª„ Step 5: (Optional) Integrate with your App UI / API

You can:

* Expose this as an **API endpoint** (`/ask`) in your backend (Express, FastAPI, etc.)
* Or call it directly in a **React front-end** via your backend.
* Add **data bindings** (like `flights`, `hazmat items`, etc.) dynamically into the prompt:

  ```js
  const prompt = `Based on flight data:
  - Total Flights: ${totalFlights}
  - Cargo per Flight: ${cargoPerFlight}
  - Hazmat Items: ${hazmatCount}

  Question: Which route has the highest hazmat density?`;
  ```

---

## âœ… Summary

| Step | Description                    | API Used             |
| ---- | ------------------------------ | -------------------- |
| 1    | Retrieve relevant info from KB | `RetrieveCommand`    |
| 2    | Feed prompt + context to LLM   | `InvokeModelCommand` |
| 3    | Return combined response       | Your app logic       |

---

Would you like me to generate a **ready-to-run Python** or **React backend** version of this too (for easy testing)?
